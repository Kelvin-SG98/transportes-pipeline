# ğŸšš Transportes Pipeline

Pipeline de dados para processamento, transformaÃ§Ã£o e agregaÃ§Ã£o de informaÃ§Ãµes de transportes utilizando **PySpark**, com ambiente totalmente orquestrado via **Docker Compose** e cobertura de testes automatizada.

---

## ğŸ“¦ Estrutura do Projeto

```
transportes-pipeline/
â”‚
â”œâ”€â”€ docker_version/
â”‚   â”œâ”€â”€ data/                # Dados de entrada (CSV)
â”‚   â”‚   â”œâ”€â”€ info_transportes.csv
â”‚   â”‚   â””â”€â”€ sample.csv       # csv para testes
â”‚   â”‚
â”‚   â”œâ”€â”€ src/                 # CÃ³digo-fonte do pipeline
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”œâ”€â”€ date_transform.py
â”‚   â”‚   â”œâ”€â”€ create_ref.py
â”‚   â”‚   â”œâ”€â”€ aggregate.py
â”‚   â”‚   â”œâ”€â”€ transform_facada.py
â”‚   â”‚   â””â”€â”€ consulta_gold.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/               # Testes unitÃ¡rios (pytest)
â”‚   â”œâ”€â”€ coverage_html/       # RelatÃ³rio de cobertura de testes (gerado automaticamente)
â”‚   â”œâ”€â”€ Dockerfile           # Dockerfile principal (Spark + Python)
â”‚   â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o dos serviÃ§os
â”‚   â””â”€â”€ requirements.txt     # DependÃªncias Python
â”‚
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente do pipeline
â””â”€â”€ .gitignore
```

---

## ğŸ³ ServiÃ§os Docker

- **spark**: Executa o pipeline principal com Spark.
- **tests**: Roda os testes unitÃ¡rios com cobertura.
- **coverage**: Servidor HTTP para visualizar o relatÃ³rio de cobertura em [http://localhost:8081](http://localhost:8081).

---

## âš™ï¸ Como rodar o projeto

### 1. **PreparaÃ§Ã£o**

- Certifique-se de ter [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/) instalados.
- Coloque seus arquivos de dados (ex: `info_transportes.csv`) na pasta /docker_version/data.

### 2. **ConfiguraÃ§Ã£o**

Edite o arquivo `.env` conforme necessÃ¡rio, por exemplo:

```
EXPECTED_COLUMNS=DATA_INICIO,DATA_FIM,CATEGORIA,DISTANCIA,PROPOSITO
EXPECTED_DATA_FORMAT=MM-dd-yyyy HH:mm
EXPECTED_DEFAULT_DATA_REF=yyyy-MM-dd
DATE_COLUMNS=DATA_INICIO,DATA_FIM
AGG_COLUMN=DT_REF
```

### 3. **Executando os testes e cobertura**

```bash
docker-compose run --rm tests
docker-compose up -d coverage
```

Acesse o relatÃ³rio de cobertura em: [http://localhost:8081](http://localhost:8081)

### 4. **Executando o pipeline Spark**

```bash
docker-compose up spark
```

### 5. **Consultar a tabela gold**

Para subir o continer e abrir o terminal
```bash
docker-compose run spark bash
```

Para executar o script de consulta (dentro do terminal do container)
```bash
python src/consulta_gold.py
```

Para sair do container
```bash
exit
```

### 6. **Remover os container**

Para subir o continer e abrir o terminal
```bash
docker-compose down -v
```

---

## ğŸ§ª Testes

- Os testes estÃ£o localizados em tests.
- SÃ£o executados automaticamente pelo serviÃ§o `tests` e geram um relatÃ³rio HTML em coverage_html.

### Exemplo de execuÃ§Ã£o manual dos testes:

```bash
docker-compose run --rm tests
```

---

## ğŸ“ Principais Componentes

- **main.py**: Ponto de entrada do pipeline.
- **ingest.py**: Leitura dos dados CSV.
- **date_transform.py**: TransformaÃ§Ã£o e padronizaÃ§Ã£o de colunas de data.
- **create_ref.py**: CriaÃ§Ã£o de coluna de referÃªncia para agregaÃ§Ã£o.
- **aggregate.py**: AgregaÃ§Ã£o dos dados para geraÃ§Ã£o do nÃ­vel gold.
- **transform_facada.py**: Orquestra as transformaÃ§Ãµes do pipeline.
- **consulta_gold.py**: Consulta os dados gerados na tabela info_corridas_do_dia.
---

## ğŸ—‚ï¸ Volumes e RelatÃ³rios

- Os dados de entrada e saÃ­da sÃ£o persistidos via volumes Docker.
- O relatÃ³rio de cobertura de testes Ã© gerado em coverage_html e exposto via HTTP na porta 8081.

---

## ğŸ› ï¸ Dicas Ãšteis

- Para limpar containers, volumes e imagens:
  ```bash
  docker-compose down -v --rmi all --remove-orphans
  ```
- Para reconstruir tudo do zero:
  ```bash
  docker-compose build --no-cache
  ```

---

## ğŸ‘©â€ğŸ’» Desenvolvimento

- Adicione novos testes em tests.
- Adicione novas dependÃªncias Python em `requirements.txt`.
- O pipeline pode ser facilmente adaptado para outros formatos de dados ou regras de negÃ³cio.

---

## ğŸ“¢ ObservaÃ§Ãµes

- Sempre rode os testes antes de executar o pipeline para garantir a integridade dos dados.
- O pipeline foi projetado para ser modular e facilmente extensÃ­vel.
- O relatÃ³rio de cobertura Ã© atualizado a cada execuÃ§Ã£o dos testes.

---
